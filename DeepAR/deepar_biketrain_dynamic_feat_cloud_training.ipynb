{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepAR Model - Predict Bike Rental with Dynamic Features\n",
    "\n",
    "Note: This dataset is not a true timeseries as there a lot of gaps\n",
    "\n",
    "We have data only for first 20 days of each month and model needs to predict the rentals for \n",
    "the remaining days of the month. The dataset consists of two years data. DeepAR will shine with true multiple-timeseries dataset like the electricity example given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_categories = False\n",
    "# Set a good base job name\n",
    "# It will help in identifying trained models and endpoints\n",
    "base_job_name = 'deepar-biketrain-with-dynamic-feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'radcv-ml-sagemaker'\n",
    "prefix = 'deepar/bikerental'\n",
    "\n",
    "# This structure allows multiple training and test files for model development and testing\n",
    "s3_data_path = \"{}/{}/data_dynamic\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('radcv-ml-sagemaker/deepar/bikerental/data_dynamic',\n",
       " 'radcv-ml-sagemaker/deepar/bikerental/output')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_data_path,s3_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name is referred as key name in S3\n",
    "# Files stored in S3 are automatically replicated across\n",
    "# three different availability zones in the region where the bucket was created.\n",
    "# http://boto3.readthedocs.io/en/latest/guide/s3.html\n",
    "def write_to_s3(filename, bucket, key):\n",
    "    with open(filename,'rb') as f: # Read in binary mode\n",
    "        return boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_fileobj(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload one or more training files and test files to S3\n",
    "write_to_s3('train_dynamic_feat.json',bucket,'deepar/bikerental/data_dynamic/train/train_dynamic_feat.json')\n",
    "write_to_s3('test_dynamic_feat.json',bucket,'deepar/bikerental/data_dynamic/test/test_dynamic_feat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint uri: s3://radcv-ml-sagemaker/deepar/bikerental/checkpoints/deepar-biketrain-with-dynamic-feat\n"
     ]
    }
   ],
   "source": [
    "# Use Spot Instance - Save up to 90% of training cost by using spot instances when compared to on-demand instances\n",
    "# Reference: https://github.com/aws-samples/amazon-sagemaker-managed-spot-training/blob/main/xgboost_built_in_managed_spot_training_checkpointing/xgboost_built_in_managed_spot_training_checkpointing.ipynb\n",
    "\n",
    "# if you are still on two-month free-tier you can use the on-demand instance by setting:\n",
    "#   use_spot_instances = False\n",
    "\n",
    "# We will use spot for training\n",
    "use_spot_instances = True\n",
    "max_run = 3600 # in seconds\n",
    "max_wait = 3600 if use_spot_instances else None # in seconds\n",
    "\n",
    "job_name = base_job_name\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    checkpoint_s3_uri = f's3://{bucket}/{prefix}/checkpoints/{job_name}'\n",
    "    \n",
    "print (f'Checkpoint uri: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a session with AWS\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::787216866617:role/service-role/AmazonSageMaker-ExecutionRole-20220603T155586\n"
     ]
    }
   ],
   "source": [
    "# This role contains the permissions needed to train, deploy models\n",
    "# SageMaker Service is trusted to assume this role\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DeepAR Container 522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1\n"
     ]
    }
   ],
   "source": [
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/image_uris.html#sagemaker.image_uris.retrieve\n",
    "\n",
    "# SDK 2 uses image_uris.retrieve the container image location\n",
    "\n",
    "# Use DeepAR Container\n",
    "container = sagemaker.image_uris.retrieve(\"forecasting-deepar\",sess.boto_region_name)\n",
    "\n",
    "print (f'Using DeepAR Container {container}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'522234722520.dkr.ecr.us-east-1.amazonaws.com/forecasting-deepar:1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training job\n",
    "# Specify type and number of instances to use\n",
    "#   Reference: http://sagemaker.readthedocs.io/en/latest/estimators.html\n",
    "# SDK 2.x version does not require train prefix for instance count and type\n",
    "\n",
    "\n",
    "# With Dynamic Feat - Using a large instance ml.c5.4xlarge = 16 CPU, 32 GB\n",
    "# Smaller instances are running into out of memory error \n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.4xlarge', # using larger instance for this dynamic feature training - resource intensive\n",
    "    output_path=\"s3://\" + s3_output_path,\n",
    "    sagemaker_session=sess,\n",
    "    base_job_name = job_name,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('H', 288, 288)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq, context_length, prediction_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"cardinality\" : \"auto\" if with_categories else ''\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time_freq': 'H',\n",
       " 'epochs': '400',\n",
       " 'early_stopping_patience': '10',\n",
       " 'mini_batch_size': '64',\n",
       " 'learning_rate': '5E-4',\n",
       " 'context_length': '288',\n",
       " 'prediction_length': '288',\n",
       " 'cardinality': ''}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we are simply referring to train path and test path\n",
    "# You can have multiple files in each path\n",
    "# SageMaker will use all the files\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://radcv-ml-sagemaker/deepar/bikerental/data_dynamic/train/',\n",
       " 'test': 's3://radcv-ml-sagemaker/deepar/bikerental/data_dynamic/test/'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-04 16:51:30 Starting - Starting the training job...\n",
      "2022-07-04 16:51:53 Starting - Preparing the instances for trainingProfilerReport-1656953490: InProgress\n",
      "......\n",
      "2022-07-04 16:53:01 Downloading - Downloading input data\n",
      "2022-07-04 16:53:01 Training - Downloading the training image......\n",
      "2022-07-04 16:53:54 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '288', 'early_stopping_patience': '10', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '288', 'epochs': '400', 'prediction_length': '288', 'time_freq': 'H'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] random_seed is None\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train_dynamic_feat.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Training set statistics:\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Integer time series\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] number of time series: 3\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] number of observations: 50904\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] mean target length: 16968.0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] min/mean/max target: 0.0/79.57296086751532/977.0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] mean abs(target): 79.57296086751532\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] contains missing values: yes (37.5%)\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Small number of time series. Doing 214 passes over dataset with prob 0.9968847352024922 per epoch.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Test set statistics:\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Integer time series\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] number of time series: 3\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] number of observations: 51768\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] mean target length: 17256.0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] min/mean/max target: 0.0/80.57008190387884/977.0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] mean abs(target): 80.57008190387884\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] contains missing values: yes (36.9%)\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/core/date_feature_set.py:44: FutureWarning: weekofyear and week have been deprecated, please use DatetimeIndex.isocalendar().week instead, which returns a Series.  To exactly reproduce the behavior of week and weekofyear and return an Index, you may call pd.Int64Index(idx.isocalendar().week)\n",
      "  return index.weekofyear / 51.0 - 0.5\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] #memory_usage::<batchbuffer> = 771.650390625 mb\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:45 INFO 140283442546496] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953625.9423516, \"EndTime\": 1656953637.3436887, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 11399.93667602539, \"count\": 1, \"min\": 11399.93667602539, \"max\": 11399.93667602539}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:53:57 INFO 140283442546496] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:00 INFO 140283442546496] #memory_usage::<model> = 263 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953637.3437762, \"EndTime\": 1656953640.2655232, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 14323.037385940552, \"count\": 1, \"min\": 14323.037385940552, \"max\": 14323.037385940552}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:12 INFO 140283442546496] Epoch[0] Batch[0] avg_epoch_loss=3.725367\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:12 INFO 140283442546496] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=3.7253670692443848\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:16 INFO 140283442546496] Epoch[0] Batch[5] avg_epoch_loss=3.725957\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:16 INFO 140283442546496] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=3.7259565591812134\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:16 INFO 140283442546496] Epoch[0] Batch [5]#011Speed: 81.11 samples/sec#011loss=3.725957\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:19 INFO 140283442546496] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953640.2655933, \"EndTime\": 1656953659.9511006, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 19685.44030189514, \"count\": 1, \"min\": 19685.44030189514, \"max\": 19685.44030189514}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:19 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.596785778056308 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:19 INFO 140283442546496] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:19 INFO 140283442546496] #quality_metric: host=algo-1, epoch=0, train loss <loss>=3.6988345623016357\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:19 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:20 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_f0f46a99-4147-4663-81d2-4f86e0729150-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953659.951177, \"EndTime\": 1656953660.135939, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 184.39221382141113, \"count\": 1, \"min\": 184.39221382141113, \"max\": 184.39221382141113}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:31 INFO 140283442546496] Epoch[1] Batch[0] avg_epoch_loss=3.674967\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:31 INFO 140283442546496] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=3.674966812133789\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:35 INFO 140283442546496] Epoch[1] Batch[5] avg_epoch_loss=3.628934\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:35 INFO 140283442546496] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=3.6289335091908774\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:35 INFO 140283442546496] Epoch[1] Batch [5]#011Speed: 85.87 samples/sec#011loss=3.628934\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] Epoch[1] Batch[10] avg_epoch_loss=3.559196\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=3.4755107402801513\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] Epoch[1] Batch [10]#011Speed: 73.24 samples/sec#011loss=3.475511\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953660.136015, \"EndTime\": 1656953679.6340501, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19497.969150543213, \"count\": 1, \"min\": 19497.969150543213, \"max\": 19497.969150543213}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.490495822682355 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] #quality_metric: host=algo-1, epoch=1, train loss <loss>=3.559195886958729\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:39 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_88df848d-7f6e-43a5-a7df-8562508cfbae-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953679.6341176, \"EndTime\": 1656953679.8703194, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 235.8710765838623, \"count\": 1, \"min\": 235.8710765838623, \"max\": 235.8710765838623}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:51 INFO 140283442546496] Epoch[2] Batch[0] avg_epoch_loss=3.561634\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:51 INFO 140283442546496] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=3.561634063720703\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:55 INFO 140283442546496] Epoch[2] Batch[5] avg_epoch_loss=3.510342\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:55 INFO 140283442546496] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=3.510342081387838\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:55 INFO 140283442546496] Epoch[2] Batch [5]#011Speed: 76.63 samples/sec#011loss=3.510342\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:58 INFO 140283442546496] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953679.8703904, \"EndTime\": 1656953698.9867496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19116.299629211426, \"count\": 1, \"min\": 19116.299629211426, \"max\": 19116.299629211426}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:58 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.014396808799845 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:58 INFO 140283442546496] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:58 INFO 140283442546496] #quality_metric: host=algo-1, epoch=2, train loss <loss>=3.5136307001113893\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:58 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:54:59 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_90af2d1b-4233-4c50-a272-e7711d163d96-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953698.9868188, \"EndTime\": 1656953699.2248714, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 237.69211769104004, \"count\": 1, \"min\": 237.69211769104004, \"max\": 237.69211769104004}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:10 INFO 140283442546496] Epoch[3] Batch[0] avg_epoch_loss=3.451152\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=3.4511523246765137\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:14 INFO 140283442546496] Epoch[3] Batch[5] avg_epoch_loss=3.507778\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:14 INFO 140283442546496] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=3.5077781677246094\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:14 INFO 140283442546496] Epoch[3] Batch [5]#011Speed: 77.60 samples/sec#011loss=3.507778\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953699.2249348, \"EndTime\": 1656953718.0515354, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18826.541662216187, \"count\": 1, \"min\": 18826.541662216187, \"max\": 18826.541662216187}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.994380903709384 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] #quality_metric: host=algo-1, epoch=3, train loss <loss>=3.447668123245239\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:18 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_afb066d5-35c7-45ce-b3e2-be8bac06b63a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953718.0516033, \"EndTime\": 1656953718.2301111, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 178.1632900238037, \"count\": 1, \"min\": 178.1632900238037, \"max\": 178.1632900238037}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:29 INFO 140283442546496] Epoch[4] Batch[0] avg_epoch_loss=3.621751\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:29 INFO 140283442546496] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=3.621751070022583\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:33 INFO 140283442546496] Epoch[4] Batch[5] avg_epoch_loss=3.373039\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:33 INFO 140283442546496] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=3.3730388879776\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:33 INFO 140283442546496] Epoch[4] Batch [5]#011Speed: 83.11 samples/sec#011loss=3.373039\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] Epoch[4] Batch[10] avg_epoch_loss=3.251722\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=3.1061425685882567\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] Epoch[4] Batch [10]#011Speed: 72.56 samples/sec#011loss=3.106143\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953718.2301772, \"EndTime\": 1656953737.51828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19288.042545318604, \"count\": 1, \"min\": 19288.042545318604, \"max\": 19288.042545318604}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.90686514080953 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] #quality_metric: host=algo-1, epoch=4, train loss <loss>=3.251722379164262\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:37 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_3d1d37b1-25d4-4953-9c0d-5e5c10b41da6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953737.518338, \"EndTime\": 1656953737.7539802, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 235.27932167053223, \"count\": 1, \"min\": 235.27932167053223, \"max\": 235.27932167053223}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:48 INFO 140283442546496] Epoch[5] Batch[0] avg_epoch_loss=3.324696\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=3.324695587158203\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:53 INFO 140283442546496] Epoch[5] Batch[5] avg_epoch_loss=3.302915\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:53 INFO 140283442546496] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=3.30291481812795\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:53 INFO 140283442546496] Epoch[5] Batch [5]#011Speed: 74.92 samples/sec#011loss=3.302915\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] Epoch[5] Batch[10] avg_epoch_loss=3.226337\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=3.1344440460205076\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] Epoch[5] Batch [10]#011Speed: 67.79 samples/sec#011loss=3.134444\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953737.7540426, \"EndTime\": 1656953757.891291, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20137.19153404236, \"count\": 1, \"min\": 20137.19153404236, \"max\": 20137.19153404236}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.12948658821709 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] #quality_metric: host=algo-1, epoch=5, train loss <loss>=3.226337194442749\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:57 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:55:58 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_4ada3844-d378-4f47-8e4d-66160d8252cb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953757.8913383, \"EndTime\": 1656953758.0644906, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 172.7900505065918, \"count\": 1, \"min\": 172.7900505065918, \"max\": 172.7900505065918}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:09 INFO 140283442546496] Epoch[6] Batch[0] avg_epoch_loss=3.296555\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:09 INFO 140283442546496] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=3.2965548038482666\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:12 INFO 140283442546496] Epoch[6] Batch[5] avg_epoch_loss=3.165051\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:12 INFO 140283442546496] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=3.1650506258010864\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:12 INFO 140283442546496] Epoch[6] Batch [5]#011Speed: 89.07 samples/sec#011loss=3.165051\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:15 INFO 140283442546496] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953758.0645385, \"EndTime\": 1656953775.859395, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 17794.801712036133, \"count\": 1, \"min\": 17794.801712036133, \"max\": 17794.801712036133}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:15 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.57199413574015 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:15 INFO 140283442546496] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:15 INFO 140283442546496] #quality_metric: host=algo-1, epoch=6, train loss <loss>=3.163175892829895\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:15 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:16 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_55307914-696b-409e-a534-8bca5830813b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953775.8594606, \"EndTime\": 1656953776.102046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 242.23923683166504, \"count\": 1, \"min\": 242.23923683166504, \"max\": 242.23923683166504}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:26 INFO 140283442546496] Epoch[7] Batch[0] avg_epoch_loss=3.075737\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:26 INFO 140283442546496] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=3.0757369995117188\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:30 INFO 140283442546496] Epoch[7] Batch[5] avg_epoch_loss=3.100948\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=3.100948452949524\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:30 INFO 140283442546496] Epoch[7] Batch [5]#011Speed: 84.78 samples/sec#011loss=3.100948\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] Epoch[7] Batch[10] avg_epoch_loss=3.088265\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=3.0730454444885256\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] Epoch[7] Batch [10]#011Speed: 64.96 samples/sec#011loss=3.073045\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953776.1021106, \"EndTime\": 1656953795.3461099, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19243.938446044922, \"count\": 1, \"min\": 19243.938446044922, \"max\": 19243.938446044922}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.491526904975935 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] #quality_metric: host=algo-1, epoch=7, train loss <loss>=3.0882652672854336\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:35 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_dd89f32e-4319-4e4d-b5f7-97bd1dfceb1d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953795.3461711, \"EndTime\": 1656953795.5857925, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 239.30978775024414, \"count\": 1, \"min\": 239.30978775024414, \"max\": 239.30978775024414}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:46 INFO 140283442546496] Epoch[8] Batch[0] avg_epoch_loss=2.974363\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=2.974363088607788\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:50 INFO 140283442546496] Epoch[8] Batch[5] avg_epoch_loss=3.112191\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=3.1121906836827598\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:50 INFO 140283442546496] Epoch[8] Batch [5]#011Speed: 82.42 samples/sec#011loss=3.112191\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953795.5858603, \"EndTime\": 1656953814.3583376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18772.417783737183, \"count\": 1, \"min\": 18772.417783737183, \"max\": 18772.417783737183}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.642004584491325 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.048387813568115\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:56:54 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_02f63f3c-c0f7-4cfb-a80f-1c754e52c6af-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953814.3584046, \"EndTime\": 1656953814.598654, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 239.8989200592041, \"count\": 1, \"min\": 239.8989200592041, \"max\": 239.8989200592041}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:06 INFO 140283442546496] Epoch[9] Batch[0] avg_epoch_loss=2.989901\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=2.989901065826416\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:10 INFO 140283442546496] Epoch[9] Batch[5] avg_epoch_loss=3.008329\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.008329192797343\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:10 INFO 140283442546496] Epoch[9] Batch [5]#011Speed: 76.46 samples/sec#011loss=3.008329\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:13 INFO 140283442546496] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953814.5987148, \"EndTime\": 1656953833.7789547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19180.181741714478, \"count\": 1, \"min\": 19180.181741714478, \"max\": 19180.181741714478}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:13 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.211193223164095 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:13 INFO 140283442546496] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:13 INFO 140283442546496] #quality_metric: host=algo-1, epoch=9, train loss <loss>=2.9953764200210573\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:13 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:14 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_6f33feb4-4713-45c8-a2b9-b4876a2e6d34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953833.779022, \"EndTime\": 1656953834.0205176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 241.14394187927246, \"count\": 1, \"min\": 241.14394187927246, \"max\": 241.14394187927246}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:25 INFO 140283442546496] Epoch[10] Batch[0] avg_epoch_loss=2.882620\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=2.882620096206665\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:29 INFO 140283442546496] Epoch[10] Batch[5] avg_epoch_loss=2.979355\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:29 INFO 140283442546496] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=2.979354898134867\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:29 INFO 140283442546496] Epoch[10] Batch [5]#011Speed: 74.57 samples/sec#011loss=2.979355\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] Epoch[10] Batch[10] avg_epoch_loss=2.950060\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=2.914906120300293\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] Epoch[10] Batch [10]#011Speed: 63.24 samples/sec#011loss=2.914906\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953834.0205827, \"EndTime\": 1656953854.3888674, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20368.223428726196, \"count\": 1, \"min\": 20368.223428726196, \"max\": 20368.223428726196}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.501455919021545 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] #quality_metric: host=algo-1, epoch=10, train loss <loss>=2.950059999119152\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:34 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_cb6714bc-f1fc-4098-aad3-7c4ad87b459f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953854.388932, \"EndTime\": 1656953854.6243842, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 235.12840270996094, \"count\": 1, \"min\": 235.12840270996094, \"max\": 235.12840270996094}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:45 INFO 140283442546496] Epoch[11] Batch[0] avg_epoch_loss=2.931838\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:45 INFO 140283442546496] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=2.931837797164917\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:49 INFO 140283442546496] Epoch[11] Batch[5] avg_epoch_loss=2.948257\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:49 INFO 140283442546496] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=2.948256731033325\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:49 INFO 140283442546496] Epoch[11] Batch [5]#011Speed: 87.13 samples/sec#011loss=2.948257\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] Epoch[11] Batch[10] avg_epoch_loss=2.869484\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=2.7749566555023195\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] Epoch[11] Batch [10]#011Speed: 68.86 samples/sec#011loss=2.774957\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953854.6244488, \"EndTime\": 1656953874.0387247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19414.215087890625, \"count\": 1, \"min\": 19414.215087890625, \"max\": 19414.215087890625}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.48046551335246 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=11, train loss <loss>=2.8694839694283227\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:57:54 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_86f0753d-a8e2-4336-ad1c-25ab4af03e22-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953874.0387852, \"EndTime\": 1656953874.2763464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 237.2438907623291, \"count\": 1, \"min\": 237.2438907623291, \"max\": 237.2438907623291}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:05 INFO 140283442546496] Epoch[12] Batch[0] avg_epoch_loss=2.807798\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=2.807798147201538\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:09 INFO 140283442546496] Epoch[12] Batch[5] avg_epoch_loss=2.930528\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:09 INFO 140283442546496] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=2.930528442064921\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:09 INFO 140283442546496] Epoch[12] Batch [5]#011Speed: 86.63 samples/sec#011loss=2.930528\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] Epoch[12] Batch[10] avg_epoch_loss=2.913944\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=2.894043445587158\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] Epoch[12] Batch [10]#011Speed: 76.13 samples/sec#011loss=2.894043\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953874.2764072, \"EndTime\": 1656953893.453069, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19176.602125167847, \"count\": 1, \"min\": 19176.602125167847, \"max\": 19176.602125167847}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.63457218257351 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] #quality_metric: host=algo-1, epoch=12, train loss <loss>=2.913944352756847\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:13 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:24 INFO 140283442546496] Epoch[13] Batch[0] avg_epoch_loss=2.876070\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:24 INFO 140283442546496] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=2.876070499420166\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:29 INFO 140283442546496] Epoch[13] Batch[5] avg_epoch_loss=2.864188\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:29 INFO 140283442546496] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=2.8641881942749023\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:29 INFO 140283442546496] Epoch[13] Batch [5]#011Speed: 73.54 samples/sec#011loss=2.864188\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:32 INFO 140283442546496] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953893.453133, \"EndTime\": 1656953912.6583002, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19204.77294921875, \"count\": 1, \"min\": 19204.77294921875, \"max\": 19204.77294921875}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:32 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.168664086655674 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:32 INFO 140283442546496] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:32 INFO 140283442546496] #quality_metric: host=algo-1, epoch=13, train loss <loss>=2.8721562385559083\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:32 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:43 INFO 140283442546496] Epoch[14] Batch[0] avg_epoch_loss=2.805722\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:43 INFO 140283442546496] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=2.8057217597961426\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:48 INFO 140283442546496] Epoch[14] Batch[5] avg_epoch_loss=2.769513\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=2.7695128520329795\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:48 INFO 140283442546496] Epoch[14] Batch [5]#011Speed: 76.74 samples/sec#011loss=2.769513\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:51 INFO 140283442546496] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953912.658369, \"EndTime\": 1656953931.680051, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19021.349668502808, \"count\": 1, \"min\": 19021.349668502808, \"max\": 19021.349668502808}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:51 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.59478353304362 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:51 INFO 140283442546496] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:51 INFO 140283442546496] #quality_metric: host=algo-1, epoch=14, train loss <loss>=2.871043157577515\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:58:51 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:02 INFO 140283442546496] Epoch[15] Batch[0] avg_epoch_loss=2.947212\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=2.947211980819702\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:07 INFO 140283442546496] Epoch[15] Batch[5] avg_epoch_loss=2.895154\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:07 INFO 140283442546496] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=2.895153760910034\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:07 INFO 140283442546496] Epoch[15] Batch [5]#011Speed: 77.14 samples/sec#011loss=2.895154\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:10 INFO 140283442546496] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953931.6801188, \"EndTime\": 1656953950.692311, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19011.768579483032, \"count\": 1, \"min\": 19011.768579483032, \"max\": 19011.768579483032}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:10 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.40019987075743 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:10 INFO 140283442546496] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=15, train loss <loss>=2.9053797245025637\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:10 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:21 INFO 140283442546496] Epoch[16] Batch[0] avg_epoch_loss=3.010283\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.0102832317352295\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:25 INFO 140283442546496] Epoch[16] Batch[5] avg_epoch_loss=2.883022\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=2.883021831512451\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:25 INFO 140283442546496] Epoch[16] Batch [5]#011Speed: 76.61 samples/sec#011loss=2.883022\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] Epoch[16] Batch[10] avg_epoch_loss=2.898698\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=2.9175090312957765\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] Epoch[16] Batch [10]#011Speed: 62.20 samples/sec#011loss=2.917509\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953950.6923766, \"EndTime\": 1656953970.958376, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20265.67769050598, \"count\": 1, \"min\": 20265.67769050598, \"max\": 20265.67769050598}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.81395997559034 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=16, train loss <loss>=2.8986978314139624\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:30 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:42 INFO 140283442546496] Epoch[17] Batch[0] avg_epoch_loss=2.819779\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:42 INFO 140283442546496] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=2.81977915763855\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:46 INFO 140283442546496] Epoch[17] Batch[5] avg_epoch_loss=2.856114\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=2.856114149093628\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:46 INFO 140283442546496] Epoch[17] Batch [5]#011Speed: 73.78 samples/sec#011loss=2.856114\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953970.9584363, \"EndTime\": 1656953990.2828836, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19324.159145355225, \"count\": 1, \"min\": 19324.159145355225, \"max\": 19324.159145355225}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.54977075090173 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=17, train loss <loss>=2.846910095214844\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 16:59:50 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_9e4c8286-4be0-474e-abfc-e260146417fd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953990.2829478, \"EndTime\": 1656953990.51972, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 236.42563819885254, \"count\": 1, \"min\": 236.42563819885254, \"max\": 236.42563819885254}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:01 INFO 140283442546496] Epoch[18] Batch[0] avg_epoch_loss=2.712598\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:01 INFO 140283442546496] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=2.7125980854034424\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:05 INFO 140283442546496] Epoch[18] Batch[5] avg_epoch_loss=2.786455\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=2.786454995473226\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:05 INFO 140283442546496] Epoch[18] Batch [5]#011Speed: 83.37 samples/sec#011loss=2.786455\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] Epoch[18] Batch[10] avg_epoch_loss=2.856496\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=2.940545415878296\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] Epoch[18] Batch [10]#011Speed: 65.38 samples/sec#011loss=2.940545\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656953990.519783, \"EndTime\": 1656954010.230564, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19710.721015930176, \"count\": 1, \"min\": 19710.721015930176, \"max\": 19710.721015930176}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.281223594433236 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=18, train loss <loss>=2.8564960956573486\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:10 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:21 INFO 140283442546496] Epoch[19] Batch[0] avg_epoch_loss=2.658118\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=2.6581175327301025\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:25 INFO 140283442546496] Epoch[19] Batch[5] avg_epoch_loss=2.791780\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=2.79177995522817\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:25 INFO 140283442546496] Epoch[19] Batch [5]#011Speed: 73.97 samples/sec#011loss=2.791780\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] Epoch[19] Batch[10] avg_epoch_loss=2.797727\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=2.8048644065856934\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] Epoch[19] Batch [10]#011Speed: 64.30 samples/sec#011loss=2.804864\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954010.230627, \"EndTime\": 1656954030.6121664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20381.237506866455, \"count\": 1, \"min\": 20381.237506866455, \"max\": 20381.237506866455}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.891932821864874 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=19, train loss <loss>=2.797727433117953\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:30 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_d9d57710-c02a-44ce-a5a7-388af93279a9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954030.6122305, \"EndTime\": 1656954030.8514278, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 238.88087272644043, \"count\": 1, \"min\": 238.88087272644043, \"max\": 238.88087272644043}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:42 INFO 140283442546496] Epoch[20] Batch[0] avg_epoch_loss=2.766193\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:42 INFO 140283442546496] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=2.76619291305542\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:46 INFO 140283442546496] Epoch[20] Batch[5] avg_epoch_loss=2.755326\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=2.755326231320699\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:46 INFO 140283442546496] Epoch[20] Batch [5]#011Speed: 74.76 samples/sec#011loss=2.755326\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] Epoch[20] Batch[10] avg_epoch_loss=2.772104\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=2.7922363758087156\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] Epoch[20] Batch [10]#011Speed: 68.99 samples/sec#011loss=2.792236\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954030.8514893, \"EndTime\": 1656954051.1684585, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20316.909551620483, \"count\": 1, \"min\": 20316.909551620483, \"max\": 20316.909551620483}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.64836264978064 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] #quality_metric: host=algo-1, epoch=20, train loss <loss>=2.7721035697243432\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:00:51 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_59e1341f-956b-49b1-88bc-305f744241e5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954051.1685243, \"EndTime\": 1656954051.4089386, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 240.02790451049805, \"count\": 1, \"min\": 240.02790451049805, \"max\": 240.02790451049805}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:02 INFO 140283442546496] Epoch[21] Batch[0] avg_epoch_loss=2.891696\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=2.891695737838745\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:06 INFO 140283442546496] Epoch[21] Batch[5] avg_epoch_loss=2.798970\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=2.798970103263855\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:06 INFO 140283442546496] Epoch[21] Batch [5]#011Speed: 74.74 samples/sec#011loss=2.798970\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:10 INFO 140283442546496] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954051.4090035, \"EndTime\": 1656954070.3902385, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18981.173753738403, \"count\": 1, \"min\": 18981.173753738403, \"max\": 18981.173753738403}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:10 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.40134719840115 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:10 INFO 140283442546496] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=21, train loss <loss>=2.8066938161849975\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:10 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:21 INFO 140283442546496] Epoch[22] Batch[0] avg_epoch_loss=2.963410\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=2.9634101390838623\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:25 INFO 140283442546496] Epoch[22] Batch[5] avg_epoch_loss=2.787584\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=2.787583510080973\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:25 INFO 140283442546496] Epoch[22] Batch [5]#011Speed: 74.85 samples/sec#011loss=2.787584\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] Epoch[22] Batch[10] avg_epoch_loss=2.821062\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=2.861236333847046\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] Epoch[22] Batch [10]#011Speed: 66.81 samples/sec#011loss=2.861236\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954070.3903048, \"EndTime\": 1656954090.541175, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20150.546073913574, \"count\": 1, \"min\": 20150.546073913574, \"max\": 20150.546073913574}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.85258486043513 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=22, train loss <loss>=2.821062066338279\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:30 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:41 INFO 140283442546496] Epoch[23] Batch[0] avg_epoch_loss=2.735238\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=2.7352380752563477\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:45 INFO 140283442546496] Epoch[23] Batch[5] avg_epoch_loss=2.774213\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:45 INFO 140283442546496] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=2.774212598800659\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:45 INFO 140283442546496] Epoch[23] Batch [5]#011Speed: 74.70 samples/sec#011loss=2.774213\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] Epoch[23] Batch[10] avg_epoch_loss=2.874153\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=2.9940810680389403\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] Epoch[23] Batch [10]#011Speed: 66.45 samples/sec#011loss=2.994081\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954090.5412242, \"EndTime\": 1656954110.7384105, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20196.842670440674, \"count\": 1, \"min\": 20196.842670440674, \"max\": 20196.842670440674}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.48018589655577 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=23, train loss <loss>=2.874152812090787\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:01:50 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:02 INFO 140283442546496] Epoch[24] Batch[0] avg_epoch_loss=2.602611\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=2.6026110649108887\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:05 INFO 140283442546496] Epoch[24] Batch[5] avg_epoch_loss=2.728535\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=2.7285346190134683\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:05 INFO 140283442546496] Epoch[24] Batch [5]#011Speed: 83.59 samples/sec#011loss=2.728535\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954110.738463, \"EndTime\": 1656954129.4029255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18664.186239242554, \"count\": 1, \"min\": 18664.186239242554, \"max\": 18664.186239242554}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.64715025870333 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] #quality_metric: host=algo-1, epoch=24, train loss <loss>=2.7565414905548096\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:09 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_143f9249-10e2-4975-948f-31d8d2c4389b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954129.4029937, \"EndTime\": 1656954129.6439495, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 240.6024932861328, \"count\": 1, \"min\": 240.6024932861328, \"max\": 240.6024932861328}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:20 INFO 140283442546496] Epoch[25] Batch[0] avg_epoch_loss=2.758882\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:20 INFO 140283442546496] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=2.7588820457458496\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:24 INFO 140283442546496] Epoch[25] Batch[5] avg_epoch_loss=2.753380\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:24 INFO 140283442546496] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=2.753379742304484\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:24 INFO 140283442546496] Epoch[25] Batch [5]#011Speed: 80.95 samples/sec#011loss=2.753380\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] Epoch[25] Batch[10] avg_epoch_loss=2.737117\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=2.7176026344299316\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] Epoch[25] Batch [10]#011Speed: 71.69 samples/sec#011loss=2.717603\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954129.6440141, \"EndTime\": 1656954148.8479252, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19203.84979248047, \"count\": 1, \"min\": 19203.84979248047, \"max\": 19203.84979248047}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=34.836600961352296 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] #quality_metric: host=algo-1, epoch=25, train loss <loss>=2.7371174205433237\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:28 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:29 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_efe49276-7acf-44a0-bad7-ab9b115ad6dc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954148.8479874, \"EndTime\": 1656954149.0131652, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 164.8404598236084, \"count\": 1, \"min\": 164.8404598236084, \"max\": 164.8404598236084}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:40 INFO 140283442546496] Epoch[26] Batch[0] avg_epoch_loss=2.735393\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:40 INFO 140283442546496] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=2.7353928089141846\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:44 INFO 140283442546496] Epoch[26] Batch[5] avg_epoch_loss=2.742725\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:44 INFO 140283442546496] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=2.742725412050883\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:44 INFO 140283442546496] Epoch[26] Batch [5]#011Speed: 74.72 samples/sec#011loss=2.742725\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] Epoch[26] Batch[10] avg_epoch_loss=2.671576\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=2.5861956119537353\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] Epoch[26] Batch [10]#011Speed: 70.32 samples/sec#011loss=2.586196\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954149.0132158, \"EndTime\": 1656954168.9026473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19889.366626739502, \"count\": 1, \"min\": 19889.366626739502, \"max\": 19889.366626739502}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.680626851487766 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=26, train loss <loss>=2.671575502915816\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:48 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:02:49 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_fa48b0cb-48e6-403b-9bc2-d9d79694c08f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954168.9027123, \"EndTime\": 1656954169.1415558, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 238.52109909057617, \"count\": 1, \"min\": 238.52109909057617, \"max\": 238.52109909057617}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:00 INFO 140283442546496] Epoch[27] Batch[0] avg_epoch_loss=2.744752\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:00 INFO 140283442546496] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.7447524070739746\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:04 INFO 140283442546496] Epoch[27] Batch[5] avg_epoch_loss=2.721327\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:04 INFO 140283442546496] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=2.72132674853007\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:04 INFO 140283442546496] Epoch[27] Batch [5]#011Speed: 79.43 samples/sec#011loss=2.721327\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] Epoch[27] Batch[10] avg_epoch_loss=2.744996\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=2.773398780822754\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] Epoch[27] Batch [10]#011Speed: 67.83 samples/sec#011loss=2.773399\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954169.1416204, \"EndTime\": 1656954188.8678658, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19726.183891296387, \"count\": 1, \"min\": 19726.183891296387, \"max\": 19726.183891296387}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.45791099615654 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] #quality_metric: host=algo-1, epoch=27, train loss <loss>=2.744995854117654\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:08 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:20 INFO 140283442546496] Epoch[28] Batch[0] avg_epoch_loss=2.765550\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:20 INFO 140283442546496] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=2.765550136566162\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:24 INFO 140283442546496] Epoch[28] Batch[5] avg_epoch_loss=2.790892\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:24 INFO 140283442546496] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=2.7908915281295776\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:24 INFO 140283442546496] Epoch[28] Batch [5]#011Speed: 74.91 samples/sec#011loss=2.790892\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:27 INFO 140283442546496] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954188.8679283, \"EndTime\": 1656954207.9270158, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19058.760166168213, \"count\": 1, \"min\": 19058.760166168213, \"max\": 19058.760166168213}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:27 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.58018212315659 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:27 INFO 140283442546496] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:27 INFO 140283442546496] #quality_metric: host=algo-1, epoch=28, train loss <loss>=2.775058221817017\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:27 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:39 INFO 140283442546496] Epoch[29] Batch[0] avg_epoch_loss=2.686520\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:39 INFO 140283442546496] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=2.6865203380584717\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:43 INFO 140283442546496] Epoch[29] Batch[5] avg_epoch_loss=2.768773\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:43 INFO 140283442546496] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=2.768773396809896\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:43 INFO 140283442546496] Epoch[29] Batch [5]#011Speed: 75.11 samples/sec#011loss=2.768773\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:47 INFO 140283442546496] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954207.9270859, \"EndTime\": 1656954227.001458, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19074.03540611267, \"count\": 1, \"min\": 19074.03540611267, \"max\": 19074.03540611267}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:47 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.500861658139 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:47 INFO 140283442546496] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:47 INFO 140283442546496] #quality_metric: host=algo-1, epoch=29, train loss <loss>=2.7412532567977905\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:47 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:57 INFO 140283442546496] Epoch[30] Batch[0] avg_epoch_loss=2.678275\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:03:57 INFO 140283442546496] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=2.6782748699188232\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:02 INFO 140283442546496] Epoch[30] Batch[5] avg_epoch_loss=2.781423\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=2.781422813733419\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:02 INFO 140283442546496] Epoch[30] Batch [5]#011Speed: 74.39 samples/sec#011loss=2.781423\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] Epoch[30] Batch[10] avg_epoch_loss=2.763343\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=2.7416479110717775\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] Epoch[30] Batch [10]#011Speed: 69.89 samples/sec#011loss=2.741648\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954227.0015278, \"EndTime\": 1656954246.7843559, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19782.49216079712, \"count\": 1, \"min\": 19782.49216079712, \"max\": 19782.49216079712}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.85718078632631 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=30, train loss <loss>=2.7633433125235816\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:06 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:17 INFO 140283442546496] Epoch[31] Batch[0] avg_epoch_loss=2.797060\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:17 INFO 140283442546496] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=2.797060489654541\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:21 INFO 140283442546496] Epoch[31] Batch[5] avg_epoch_loss=2.729906\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=2.7299055258433023\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:21 INFO 140283442546496] Epoch[31] Batch [5]#011Speed: 77.40 samples/sec#011loss=2.729906\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] Epoch[31] Batch[10] avg_epoch_loss=2.706887\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=2.6792656898498537\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] Epoch[31] Batch [10]#011Speed: 66.01 samples/sec#011loss=2.679266\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954246.784421, \"EndTime\": 1656954266.5700293, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19785.29381752014, \"count\": 1, \"min\": 19785.29381752014, \"max\": 19785.29381752014}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.91394365879125 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] #quality_metric: host=algo-1, epoch=31, train loss <loss>=2.7068874185735528\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:26 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:37 INFO 140283442546496] Epoch[32] Batch[0] avg_epoch_loss=2.733768\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:37 INFO 140283442546496] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=2.7337684631347656\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:41 INFO 140283442546496] Epoch[32] Batch[5] avg_epoch_loss=2.715639\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=2.7156385580698648\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:41 INFO 140283442546496] Epoch[32] Batch [5]#011Speed: 80.16 samples/sec#011loss=2.715639\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] Epoch[32] Batch[10] avg_epoch_loss=2.685479\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=2.649287700653076\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] Epoch[32] Batch [10]#011Speed: 60.60 samples/sec#011loss=2.649288\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954266.570082, \"EndTime\": 1656954286.2925193, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19722.15509414673, \"count\": 1, \"min\": 19722.15509414673, \"max\": 19722.15509414673}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.239416834231854 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=32, train loss <loss>=2.68547907742587\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:46 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:57 INFO 140283442546496] Epoch[33] Batch[0] avg_epoch_loss=2.757754\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:04:57 INFO 140283442546496] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=2.757754325866699\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:01 INFO 140283442546496] Epoch[33] Batch[5] avg_epoch_loss=2.753541\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:01 INFO 140283442546496] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=2.753541032473246\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:01 INFO 140283442546496] Epoch[33] Batch [5]#011Speed: 85.91 samples/sec#011loss=2.753541\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] Epoch[33] Batch[10] avg_epoch_loss=2.666605\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=2.562281036376953\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] Epoch[33] Batch [10]#011Speed: 70.14 samples/sec#011loss=2.562281\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954286.2925713, \"EndTime\": 1656954305.6301677, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19337.286949157715, \"count\": 1, \"min\": 19337.286949157715, \"max\": 19337.286949157715}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.19992792721526 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=33, train loss <loss>=2.666604670611295\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:05 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_b0f3761a-dfc6-4528-ae48-3cfd6ea86ed9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954305.6302397, \"EndTime\": 1656954305.8253443, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 194.66519355773926, \"count\": 1, \"min\": 194.66519355773926, \"max\": 194.66519355773926}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:16 INFO 140283442546496] Epoch[34] Batch[0] avg_epoch_loss=2.710984\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:16 INFO 140283442546496] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=2.710984230041504\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:20 INFO 140283442546496] Epoch[34] Batch[5] avg_epoch_loss=2.682383\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:20 INFO 140283442546496] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=2.6823832988739014\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:20 INFO 140283442546496] Epoch[34] Batch [5]#011Speed: 87.58 samples/sec#011loss=2.682383\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] Epoch[34] Batch[10] avg_epoch_loss=2.720698\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=2.766676330566406\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] Epoch[34] Batch [10]#011Speed: 73.07 samples/sec#011loss=2.766676\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954305.8253982, \"EndTime\": 1656954324.9308867, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19105.42130470276, \"count\": 1, \"min\": 19105.42130470276, \"max\": 19105.42130470276}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.602862832659824 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] #quality_metric: host=algo-1, epoch=34, train loss <loss>=2.7206983132795854\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:24 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:36 INFO 140283442546496] Epoch[35] Batch[0] avg_epoch_loss=2.825174\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:36 INFO 140283442546496] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=2.825173854827881\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:39 INFO 140283442546496] Epoch[35] Batch[5] avg_epoch_loss=2.691557\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:39 INFO 140283442546496] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=2.6915568510691323\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:39 INFO 140283442546496] Epoch[35] Batch [5]#011Speed: 85.17 samples/sec#011loss=2.691557\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954324.9309506, \"EndTime\": 1656954343.4694834, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18538.21110725403, \"count\": 1, \"min\": 18538.21110725403, \"max\": 18538.21110725403}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=34.19944562762184 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] #quality_metric: host=algo-1, epoch=35, train loss <loss>=2.6646833419799805\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:43 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_018cdf69-5916-4b71-a1dc-a73564917465-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954343.4695535, \"EndTime\": 1656954343.708954, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 239.03918266296387, \"count\": 1, \"min\": 239.03918266296387, \"max\": 239.03918266296387}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:54 INFO 140283442546496] Epoch[36] Batch[0] avg_epoch_loss=2.762609\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=2.7626092433929443\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:59 INFO 140283442546496] Epoch[36] Batch[5] avg_epoch_loss=2.691617\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:59 INFO 140283442546496] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=2.691617488861084\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:05:59 INFO 140283442546496] Epoch[36] Batch [5]#011Speed: 78.26 samples/sec#011loss=2.691617\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:02 INFO 140283442546496] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954343.7090173, \"EndTime\": 1656954362.660171, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18951.091766357422, \"count\": 1, \"min\": 18951.091766357422, \"max\": 18951.091766357422}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:02 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.401596426044335 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:02 INFO 140283442546496] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=36, train loss <loss>=2.682506966590881\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:02 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:14 INFO 140283442546496] Epoch[37] Batch[0] avg_epoch_loss=2.558651\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:14 INFO 140283442546496] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=2.5586512088775635\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:18 INFO 140283442546496] Epoch[37] Batch[5] avg_epoch_loss=2.691472\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:18 INFO 140283442546496] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=2.6914717753728232\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:18 INFO 140283442546496] Epoch[37] Batch [5]#011Speed: 73.94 samples/sec#011loss=2.691472\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] Epoch[37] Batch[10] avg_epoch_loss=2.632969\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=2.5627647399902345\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] Epoch[37] Batch [10]#011Speed: 68.02 samples/sec#011loss=2.562765\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954362.660238, \"EndTime\": 1656954383.1306512, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20470.075845718384, \"count\": 1, \"min\": 20470.075845718384, \"max\": 20470.075845718384}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.313863239138456 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] #quality_metric: host=algo-1, epoch=37, train loss <loss>=2.6329685774716465\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:23 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_727d54e8-1d31-45c0-b59d-60d02b61407c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954383.1307137, \"EndTime\": 1656954383.3665504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 235.518217086792, \"count\": 1, \"min\": 235.518217086792, \"max\": 235.518217086792}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:34 INFO 140283442546496] Epoch[38] Batch[0] avg_epoch_loss=2.756012\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:34 INFO 140283442546496] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=2.756012439727783\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:38 INFO 140283442546496] Epoch[38] Batch[5] avg_epoch_loss=2.664054\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:38 INFO 140283442546496] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=2.6640543142954507\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:38 INFO 140283442546496] Epoch[38] Batch [5]#011Speed: 83.80 samples/sec#011loss=2.664054\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] Epoch[38] Batch[10] avg_epoch_loss=2.635459\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=2.601145029067993\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] Epoch[38] Batch [10]#011Speed: 67.87 samples/sec#011loss=2.601145\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954383.3666122, \"EndTime\": 1656954402.9577801, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19591.103553771973, \"count\": 1, \"min\": 19591.103553771973, \"max\": 19591.103553771973}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.07604861704264 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] #quality_metric: host=algo-1, epoch=38, train loss <loss>=2.6354591846466064\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:42 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:54 INFO 140283442546496] Epoch[39] Batch[0] avg_epoch_loss=2.705197\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=2.7051970958709717\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:58 INFO 140283442546496] Epoch[39] Batch[5] avg_epoch_loss=2.681426\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:58 INFO 140283442546496] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=2.681426445643107\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:06:58 INFO 140283442546496] Epoch[39] Batch [5]#011Speed: 76.76 samples/sec#011loss=2.681426\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:02 INFO 140283442546496] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954402.9578555, \"EndTime\": 1656954422.2089052, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19250.5886554718, \"count\": 1, \"min\": 19250.5886554718, \"max\": 19250.5886554718}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:02 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.271636217300316 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:02 INFO 140283442546496] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=39, train loss <loss>=2.7194043397903442\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:02 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:13 INFO 140283442546496] Epoch[40] Batch[0] avg_epoch_loss=2.652943\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:13 INFO 140283442546496] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=2.6529428958892822\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:17 INFO 140283442546496] Epoch[40] Batch[5] avg_epoch_loss=2.652596\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:17 INFO 140283442546496] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=2.6525956789652505\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:17 INFO 140283442546496] Epoch[40] Batch [5]#011Speed: 73.76 samples/sec#011loss=2.652596\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:21 INFO 140283442546496] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954422.208957, \"EndTime\": 1656954441.4423156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19233.0322265625, \"count\": 1, \"min\": 19233.0322265625, \"max\": 19233.0322265625}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:21 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.49600639546952 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:21 INFO 140283442546496] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=40, train loss <loss>=2.6639201641082764\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:21 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:32 INFO 140283442546496] Epoch[41] Batch[0] avg_epoch_loss=2.716103\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:32 INFO 140283442546496] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=2.7161030769348145\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:36 INFO 140283442546496] Epoch[41] Batch[5] avg_epoch_loss=2.686705\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:36 INFO 140283442546496] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=2.686704953511556\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:36 INFO 140283442546496] Epoch[41] Batch [5]#011Speed: 74.83 samples/sec#011loss=2.686705\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] Epoch[41] Batch[10] avg_epoch_loss=2.656490\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=2.6202319622039796\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] Epoch[41] Batch [10]#011Speed: 64.29 samples/sec#011loss=2.620232\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954441.442385, \"EndTime\": 1656954461.8139203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20371.201038360596, \"count\": 1, \"min\": 20371.201038360596, \"max\": 20371.201038360596}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.25126490752943 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=41, train loss <loss>=2.6564899574626577\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:41 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:53 INFO 140283442546496] Epoch[42] Batch[0] avg_epoch_loss=2.686593\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:53 INFO 140283442546496] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=2.6865932941436768\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:57 INFO 140283442546496] Epoch[42] Batch[5] avg_epoch_loss=2.668946\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:57 INFO 140283442546496] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=2.6689456701278687\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:07:57 INFO 140283442546496] Epoch[42] Batch [5]#011Speed: 74.67 samples/sec#011loss=2.668946\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:01 INFO 140283442546496] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954461.813984, \"EndTime\": 1656954481.0820668, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19267.772436141968, \"count\": 1, \"min\": 19267.772436141968, \"max\": 19267.772436141968}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:01 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.60702137776944 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:01 INFO 140283442546496] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:01 INFO 140283442546496] #quality_metric: host=algo-1, epoch=42, train loss <loss>=2.653824234008789\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:01 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:11 INFO 140283442546496] Epoch[43] Batch[0] avg_epoch_loss=2.543079\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:11 INFO 140283442546496] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=2.543079137802124\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:15 INFO 140283442546496] Epoch[43] Batch[5] avg_epoch_loss=2.635423\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:15 INFO 140283442546496] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=2.635423183441162\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:15 INFO 140283442546496] Epoch[43] Batch [5]#011Speed: 87.36 samples/sec#011loss=2.635423\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] Epoch[43] Batch[10] avg_epoch_loss=2.636635\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=2.6380897998809814\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] Epoch[43] Batch [10]#011Speed: 66.32 samples/sec#011loss=2.638090\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954481.0821342, \"EndTime\": 1656954500.2156074, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19133.14652442932, \"count\": 1, \"min\": 19133.14652442932, \"max\": 19133.14652442932}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.749302444702394 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] #quality_metric: host=algo-1, epoch=43, train loss <loss>=2.636635281822898\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:20 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:31 INFO 140283442546496] Epoch[44] Batch[0] avg_epoch_loss=2.610623\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:31 INFO 140283442546496] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=2.6106231212615967\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:35 INFO 140283442546496] Epoch[44] Batch[5] avg_epoch_loss=2.662661\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:35 INFO 140283442546496] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=2.662660559018453\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:35 INFO 140283442546496] Epoch[44] Batch [5]#011Speed: 85.02 samples/sec#011loss=2.662661\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:38 INFO 140283442546496] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954500.2156725, \"EndTime\": 1656954518.7176561, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18501.657009124756, \"count\": 1, \"min\": 18501.657009124756, \"max\": 18501.657009124756}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:38 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.02389677459128 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:38 INFO 140283442546496] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:38 INFO 140283442546496] #quality_metric: host=algo-1, epoch=44, train loss <loss>=2.6677471876144407\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:38 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:50 INFO 140283442546496] Epoch[45] Batch[0] avg_epoch_loss=2.538395\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=2.5383951663970947\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:54 INFO 140283442546496] Epoch[45] Batch[5] avg_epoch_loss=2.642927\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=2.6429270108540854\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:54 INFO 140283442546496] Epoch[45] Batch [5]#011Speed: 74.20 samples/sec#011loss=2.642927\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] Epoch[45] Batch[10] avg_epoch_loss=2.567263\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=2.476466679573059\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] Epoch[45] Batch [10]#011Speed: 68.47 samples/sec#011loss=2.476467\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954518.7177222, \"EndTime\": 1656954539.0101411, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20292.096614837646, \"count\": 1, \"min\": 20292.096614837646, \"max\": 20292.096614837646}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.08130499498647 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] #quality_metric: host=algo-1, epoch=45, train loss <loss>=2.567263223908164\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:08:59 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_280d6fb2-b8e4-489b-a40a-3cd94a087c48-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954539.0102086, \"EndTime\": 1656954539.248461, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 237.92767524719238, \"count\": 1, \"min\": 237.92767524719238, \"max\": 237.92767524719238}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:10 INFO 140283442546496] Epoch[46] Batch[0] avg_epoch_loss=2.687916\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=2.687915802001953\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:14 INFO 140283442546496] Epoch[46] Batch[5] avg_epoch_loss=2.681820\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:14 INFO 140283442546496] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=2.6818200747172036\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:14 INFO 140283442546496] Epoch[46] Batch [5]#011Speed: 84.52 samples/sec#011loss=2.681820\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:17 INFO 140283442546496] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954539.2485232, \"EndTime\": 1656954558.0000448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18751.461267471313, \"count\": 1, \"min\": 18751.461267471313, \"max\": 18751.461267471313}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:18 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.01060578770436 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:18 INFO 140283442546496] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:18 INFO 140283442546496] #quality_metric: host=algo-1, epoch=46, train loss <loss>=2.631531763076782\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:18 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:29 INFO 140283442546496] Epoch[47] Batch[0] avg_epoch_loss=2.652082\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:29 INFO 140283442546496] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=2.6520819664001465\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:33 INFO 140283442546496] Epoch[47] Batch[5] avg_epoch_loss=2.621041\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:33 INFO 140283442546496] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=2.621040860811869\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:33 INFO 140283442546496] Epoch[47] Batch [5]#011Speed: 73.43 samples/sec#011loss=2.621041\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:37 INFO 140283442546496] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954558.0001006, \"EndTime\": 1656954577.4466212, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19446.190357208252, \"count\": 1, \"min\": 19446.190357208252, \"max\": 19446.190357208252}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:37 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.499776806187455 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:37 INFO 140283442546496] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:37 INFO 140283442546496] #quality_metric: host=algo-1, epoch=47, train loss <loss>=2.650187277793884\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:37 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:48 INFO 140283442546496] Epoch[48] Batch[0] avg_epoch_loss=2.522491\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=2.522491455078125\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:53 INFO 140283442546496] Epoch[48] Batch[5] avg_epoch_loss=2.641114\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:53 INFO 140283442546496] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=2.6411139965057373\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:53 INFO 140283442546496] Epoch[48] Batch [5]#011Speed: 75.05 samples/sec#011loss=2.641114\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:56 INFO 140283442546496] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954577.4466884, \"EndTime\": 1656954596.5129335, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19065.919160842896, \"count\": 1, \"min\": 19065.919160842896, \"max\": 19065.919160842896}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:56 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.25288643392157 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:56 INFO 140283442546496] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:56 INFO 140283442546496] #quality_metric: host=algo-1, epoch=48, train loss <loss>=2.641266202926636\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:09:56 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:07 INFO 140283442546496] Epoch[49] Batch[0] avg_epoch_loss=2.757817\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:07 INFO 140283442546496] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=2.757816791534424\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:12 INFO 140283442546496] Epoch[49] Batch[5] avg_epoch_loss=2.703703\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:12 INFO 140283442546496] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=2.7037026484807334\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:12 INFO 140283442546496] Epoch[49] Batch [5]#011Speed: 74.22 samples/sec#011loss=2.703703\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] Epoch[49] Batch[10] avg_epoch_loss=2.638034\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=2.559231901168823\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] Epoch[49] Batch [10]#011Speed: 68.11 samples/sec#011loss=2.559232\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954596.5129974, \"EndTime\": 1656954616.7775257, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20264.19949531555, \"count\": 1, \"min\": 20264.19949531555, \"max\": 20264.19949531555}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.026797764953194 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] #quality_metric: host=algo-1, epoch=49, train loss <loss>=2.63803412697532\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:16 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:27 INFO 140283442546496] Epoch[50] Batch[0] avg_epoch_loss=2.605701\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:27 INFO 140283442546496] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=2.605701446533203\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:31 INFO 140283442546496] Epoch[50] Batch[5] avg_epoch_loss=2.643855\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:31 INFO 140283442546496] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=2.6438551346460977\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:31 INFO 140283442546496] Epoch[50] Batch [5]#011Speed: 76.94 samples/sec#011loss=2.643855\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] Epoch[50] Batch[10] avg_epoch_loss=2.620547\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=2.5925763607025147\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] Epoch[50] Batch [10]#011Speed: 62.76 samples/sec#011loss=2.592576\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954616.7775788, \"EndTime\": 1656954636.8625648, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20084.627389907837, \"count\": 1, \"min\": 20084.627389907837, \"max\": 20084.627389907837}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.30889456263357 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] #quality_metric: host=algo-1, epoch=50, train loss <loss>=2.6205466010353784\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:36 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:48 INFO 140283442546496] Epoch[51] Batch[0] avg_epoch_loss=2.547674\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:48 INFO 140283442546496] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=2.5476744174957275\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:51 INFO 140283442546496] Epoch[51] Batch[5] avg_epoch_loss=2.621859\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:51 INFO 140283442546496] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=2.6218590339024863\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:51 INFO 140283442546496] Epoch[51] Batch [5]#011Speed: 86.69 samples/sec#011loss=2.621859\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:54 INFO 140283442546496] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954636.8626313, \"EndTime\": 1656954654.9774005, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18114.46499824524, \"count\": 1, \"min\": 18114.46499824524, \"max\": 18114.46499824524}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:54 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=34.99943662671811 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:54 INFO 140283442546496] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:54 INFO 140283442546496] #quality_metric: host=algo-1, epoch=51, train loss <loss>=2.6460233449935915\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:10:54 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:06 INFO 140283442546496] Epoch[52] Batch[0] avg_epoch_loss=2.741111\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=2.7411110401153564\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:10 INFO 140283442546496] Epoch[52] Batch[5] avg_epoch_loss=2.623129\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:10 INFO 140283442546496] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=2.6231287320454917\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:10 INFO 140283442546496] Epoch[52] Batch [5]#011Speed: 74.13 samples/sec#011loss=2.623129\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] Epoch[52] Batch[10] avg_epoch_loss=2.633218\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=2.6453247547149656\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] Epoch[52] Batch [10]#011Speed: 66.98 samples/sec#011loss=2.645325\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954654.9774833, \"EndTime\": 1656954675.4128268, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 20435.003995895386, \"count\": 1, \"min\": 20435.003995895386, \"max\": 20435.003995895386}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.612297703817575 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] #quality_metric: host=algo-1, epoch=52, train loss <loss>=2.633217833258889\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:15 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:26 INFO 140283442546496] Epoch[53] Batch[0] avg_epoch_loss=2.566760\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:26 INFO 140283442546496] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=2.566760301589966\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:30 INFO 140283442546496] Epoch[53] Batch[5] avg_epoch_loss=2.561607\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.561607042948405\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:30 INFO 140283442546496] Epoch[53] Batch [5]#011Speed: 87.22 samples/sec#011loss=2.561607\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:33 INFO 140283442546496] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954675.4128816, \"EndTime\": 1656954693.3845854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 17971.37975692749, \"count\": 1, \"min\": 17971.37975692749, \"max\": 17971.37975692749}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:33 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.27813035199676 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:33 INFO 140283442546496] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:33 INFO 140283442546496] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.5952203989028932\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:33 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:43 INFO 140283442546496] Epoch[54] Batch[0] avg_epoch_loss=2.613964\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:43 INFO 140283442546496] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.6139638423919678\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:47 INFO 140283442546496] Epoch[54] Batch[5] avg_epoch_loss=2.643185\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:47 INFO 140283442546496] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.6431851387023926\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:47 INFO 140283442546496] Epoch[54] Batch [5]#011Speed: 87.07 samples/sec#011loss=2.643185\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] Epoch[54] Batch[10] avg_epoch_loss=2.673320\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=2.7094826221466066\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] Epoch[54] Batch [10]#011Speed: 69.10 samples/sec#011loss=2.709483\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954693.3846502, \"EndTime\": 1656954712.2699764, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18885.01286506653, \"count\": 1, \"min\": 18885.01286506653, \"max\": 18885.01286506653}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=36.32492078828855 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.6733203584497627\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:11:52 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:03 INFO 140283442546496] Epoch[55] Batch[0] avg_epoch_loss=2.574417\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:03 INFO 140283442546496] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=2.5744166374206543\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:07 INFO 140283442546496] Epoch[55] Batch[5] avg_epoch_loss=2.580731\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:07 INFO 140283442546496] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=2.58073095480601\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:07 INFO 140283442546496] Epoch[55] Batch [5]#011Speed: 78.52 samples/sec#011loss=2.580731\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954712.2700408, \"EndTime\": 1656954731.2595484, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18989.20750617981, \"count\": 1, \"min\": 18989.20750617981, \"max\": 18989.20750617981}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.438781278910046 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] #quality_metric: host=algo-1, epoch=55, train loss <loss>=2.538067650794983\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:11 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/state_9b102be8-4474-45be-904b-ac7429377794-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954731.2596006, \"EndTime\": 1656954731.4312823, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 171.35071754455566, \"count\": 1, \"min\": 171.35071754455566, \"max\": 171.35071754455566}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:22 INFO 140283442546496] Epoch[56] Batch[0] avg_epoch_loss=2.658847\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:22 INFO 140283442546496] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=2.658846616744995\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:27 INFO 140283442546496] Epoch[56] Batch[5] avg_epoch_loss=2.607639\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:27 INFO 140283442546496] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=2.607638716697693\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:27 INFO 140283442546496] Epoch[56] Batch [5]#011Speed: 74.17 samples/sec#011loss=2.607639\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:30 INFO 140283442546496] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954731.4313343, \"EndTime\": 1656954750.7943416, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19362.945556640625, \"count\": 1, \"min\": 19362.945556640625, \"max\": 19362.945556640625}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:30 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.27797594296881 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:30 INFO 140283442546496] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:30 INFO 140283442546496] #quality_metric: host=algo-1, epoch=56, train loss <loss>=2.6089706897735594\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:30 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:42 INFO 140283442546496] Epoch[57] Batch[0] avg_epoch_loss=2.561437\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:42 INFO 140283442546496] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=2.561436653137207\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:46 INFO 140283442546496] Epoch[57] Batch[5] avg_epoch_loss=2.610824\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:46 INFO 140283442546496] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=2.6108243465423584\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:46 INFO 140283442546496] Epoch[57] Batch [5]#011Speed: 74.26 samples/sec#011loss=2.610824\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:50 INFO 140283442546496] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954750.794416, \"EndTime\": 1656954770.162372, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19367.62833595276, \"count\": 1, \"min\": 19367.62833595276, \"max\": 19367.62833595276}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:50 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=31.39243616078205 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:50 INFO 140283442546496] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:50 INFO 140283442546496] #quality_metric: host=algo-1, epoch=57, train loss <loss>=2.622614312171936\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:12:50 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:01 INFO 140283442546496] Epoch[58] Batch[0] avg_epoch_loss=2.563475\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:01 INFO 140283442546496] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=2.5634751319885254\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:05 INFO 140283442546496] Epoch[58] Batch[5] avg_epoch_loss=2.588995\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:05 INFO 140283442546496] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=2.588995178540548\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:05 INFO 140283442546496] Epoch[58] Batch [5]#011Speed: 84.22 samples/sec#011loss=2.588995\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:08 INFO 140283442546496] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954770.1624374, \"EndTime\": 1656954788.7455404, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18582.77940750122, \"count\": 1, \"min\": 18582.77940750122, \"max\": 18582.77940750122}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:08 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.98739408618813 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:08 INFO 140283442546496] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:08 INFO 140283442546496] #quality_metric: host=algo-1, epoch=58, train loss <loss>=2.6116395473480223\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:08 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:19 INFO 140283442546496] Epoch[59] Batch[0] avg_epoch_loss=2.584482\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:19 INFO 140283442546496] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=2.584482431411743\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:23 INFO 140283442546496] Epoch[59] Batch[5] avg_epoch_loss=2.611910\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:23 INFO 140283442546496] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=2.611909786860148\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:23 INFO 140283442546496] Epoch[59] Batch [5]#011Speed: 81.62 samples/sec#011loss=2.611910\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:27 INFO 140283442546496] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954788.745592, \"EndTime\": 1656954807.4963422, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18750.36883354187, \"count\": 1, \"min\": 18750.36883354187, \"max\": 18750.36883354187}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:27 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.74587220351964 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:27 INFO 140283442546496] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:27 INFO 140283442546496] #quality_metric: host=algo-1, epoch=59, train loss <loss>=2.665965962409973\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:27 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:38 INFO 140283442546496] Epoch[60] Batch[0] avg_epoch_loss=2.508655\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:38 INFO 140283442546496] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=2.5086545944213867\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:41 INFO 140283442546496] Epoch[60] Batch[5] avg_epoch_loss=2.542560\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=2.542559583981832\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:41 INFO 140283442546496] Epoch[60] Batch [5]#011Speed: 84.60 samples/sec#011loss=2.542560\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] Epoch[60] Batch[10] avg_epoch_loss=2.540191\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=2.537349271774292\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] Epoch[60] Batch [10]#011Speed: 61.33 samples/sec#011loss=2.537349\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954807.4963992, \"EndTime\": 1656954827.0390468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19542.29712486267, \"count\": 1, \"min\": 19542.29712486267, \"max\": 19542.29712486267}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=35.66607791114919 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] #quality_metric: host=algo-1, epoch=60, train loss <loss>=2.540191260251132\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:47 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:58 INFO 140283442546496] Epoch[61] Batch[0] avg_epoch_loss=2.520549\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:13:58 INFO 140283442546496] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=2.5205485820770264\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:02 INFO 140283442546496] Epoch[61] Batch[5] avg_epoch_loss=2.614140\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:02 INFO 140283442546496] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=2.614140272140503\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:02 INFO 140283442546496] Epoch[61] Batch [5]#011Speed: 76.70 samples/sec#011loss=2.614140\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:06 INFO 140283442546496] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954827.0391026, \"EndTime\": 1656954846.0164235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18977.03218460083, \"count\": 1, \"min\": 18977.03218460083, \"max\": 18977.03218460083}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:06 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.566720054290265 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:06 INFO 140283442546496] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:06 INFO 140283442546496] #quality_metric: host=algo-1, epoch=61, train loss <loss>=2.6257090091705324\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:06 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:17 INFO 140283442546496] Epoch[62] Batch[0] avg_epoch_loss=2.741099\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:17 INFO 140283442546496] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=2.7410988807678223\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:21 INFO 140283442546496] Epoch[62] Batch[5] avg_epoch_loss=2.666313\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=2.6663126150767007\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:21 INFO 140283442546496] Epoch[62] Batch [5]#011Speed: 84.99 samples/sec#011loss=2.666313\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] Epoch[62] Batch[10] avg_epoch_loss=2.635698\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=2.5989609718322755\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] Epoch[62] Batch [10]#011Speed: 73.03 samples/sec#011loss=2.598961\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954846.0164914, \"EndTime\": 1656954865.3860168, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19369.19331550598, \"count\": 1, \"min\": 19369.19331550598, \"max\": 19369.19331550598}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.19688737684386 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] #quality_metric: host=algo-1, epoch=62, train loss <loss>=2.6356982317837803\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:25 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:36 INFO 140283442546496] Epoch[63] Batch[0] avg_epoch_loss=2.493174\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:36 INFO 140283442546496] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=2.4931740760803223\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:41 INFO 140283442546496] Epoch[63] Batch[5] avg_epoch_loss=2.600216\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:41 INFO 140283442546496] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=2.600215713183085\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:41 INFO 140283442546496] Epoch[63] Batch [5]#011Speed: 74.41 samples/sec#011loss=2.600216\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:44 INFO 140283442546496] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954865.3860793, \"EndTime\": 1656954884.7487054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 19362.319469451904, \"count\": 1, \"min\": 19362.319469451904, \"max\": 19362.319469451904}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:44 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=32.22732481204811 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:44 INFO 140283442546496] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:44 INFO 140283442546496] #quality_metric: host=algo-1, epoch=63, train loss <loss>=2.5791014671325683\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:44 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:56 INFO 140283442546496] Epoch[64] Batch[0] avg_epoch_loss=2.631697\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:14:56 INFO 140283442546496] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=2.6316967010498047\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:00 INFO 140283442546496] Epoch[64] Batch[5] avg_epoch_loss=2.567630\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:00 INFO 140283442546496] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=2.567630330721537\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:00 INFO 140283442546496] Epoch[64] Batch [5]#011Speed: 80.88 samples/sec#011loss=2.567630\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:03 INFO 140283442546496] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954884.7488005, \"EndTime\": 1656954903.4852178, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18736.01269721985, \"count\": 1, \"min\": 18736.01269721985, \"max\": 18736.01269721985}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:03 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=33.037833435643094 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:03 INFO 140283442546496] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:03 INFO 140283442546496] #quality_metric: host=algo-1, epoch=64, train loss <loss>=2.543308663368225\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:03 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:14 INFO 140283442546496] Epoch[65] Batch[0] avg_epoch_loss=2.559038\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:14 INFO 140283442546496] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=2.559037923812866\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:18 INFO 140283442546496] Epoch[65] Batch[5] avg_epoch_loss=2.587904\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:18 INFO 140283442546496] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=2.5879037380218506\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:18 INFO 140283442546496] Epoch[65] Batch [5]#011Speed: 86.27 samples/sec#011loss=2.587904\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954903.4852724, \"EndTime\": 1656954921.9138691, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 18428.252458572388, \"count\": 1, \"min\": 18428.252458572388, \"max\": 18428.252458572388}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] #throughput_metric: host=algo-1, train throughput=34.077966811816324 records/second\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] #quality_metric: host=algo-1, epoch=65, train loss <loss>=2.5414076805114747\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] loss did not improve\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:21 INFO 140283442546496] Loading parameters from best epoch (55)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954921.9139209, \"EndTime\": 1656954922.0090063, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 94.76661682128906, \"count\": 1, \"min\": 94.76661682128906, \"max\": 94.76661682128906}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] stopping training now\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] Final loss: 2.538067650794983 (occurred at epoch 55)\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] #quality_metric: host=algo-1, train final_loss <loss>=2.538067650794983\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/run_worker.py:348: DeprecationWarning: The 'warn' function is deprecated, use 'warning' instead\n",
      "  \"You are using large values for `context_length` and/or `prediction_length`. \"\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 WARNING 140283442546496] You are using large values for `context_length` and/or `prediction_length`. The following step may take some time. If the step crashes, use an instance with more memory or reduce these two parameters.\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 WARNING 140283442546496] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:22 INFO 140283442546496] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954922.0090725, \"EndTime\": 1656954934.0163896, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 12006.529569625854, \"count\": 1, \"min\": 12006.529569625854, \"max\": 12006.529569625854}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954934.0164676, \"EndTime\": 1656954935.012959, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 13003.140926361084, \"count\": 1, \"min\": 13003.140926361084, \"max\": 13003.140926361084}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954935.0130095, \"EndTime\": 1656954935.1302707, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 117.22779273986816, \"count\": 1, \"min\": 117.22779273986816, \"max\": 117.22779273986816}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] #memory_usage::<batchbuffer> = 771.650390625 mb\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:35 INFO 140283442546496] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954935.1303163, \"EndTime\": 1656954935.1354046, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.026464462280273438, \"count\": 1, \"min\": 0.026464462280273438, \"max\": 0.026464462280273438}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954935.1354463, \"EndTime\": 1656954943.4964223, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 8361.041069030762, \"count\": 1, \"min\": 8361.041069030762, \"max\": 8361.041069030762}}}\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, RMSE): 49.81359980626049\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, mean_absolute_QuantileLoss): 19408.136010169404\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, mean_wQuantileLoss): 0.16123731835315613\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.1]): 0.08636846146918695\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.2]): 0.13711759781871136\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.3]): 0.17261868207168665\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.4]): 0.1946976361154787\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.5]): 0.20371844694609484\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.6]): 0.20319205212822844\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.7]): 0.1883861155375062\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.8]): 0.15848584903775034\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #test_score (algo-1, wQuantileLoss[0.9]): 0.10655102405376168\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #quality_metric: host=algo-1, test RMSE <loss>=49.81359980626049\u001b[0m\n",
      "\u001b[34m[07/04/2022 17:15:43 INFO 140283442546496] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.16123731835315613\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1656954943.496502, \"EndTime\": 1656954943.786961, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 5.110740661621094, \"count\": 1, \"min\": 5.110740661621094, \"max\": 5.110740661621094}, \"totaltime\": {\"sum\": 1318186.192035675, \"count\": 1, \"min\": 1318186.192035675, \"max\": 1318186.192035675}}}\u001b[0m\n",
      "\n",
      "2022-07-04 17:16:08 Uploading - Uploading generated training model\n",
      "2022-07-04 17:16:08 Completed - Training job completed\n",
      "Training seconds: 1398\n",
      "Billable seconds: 610\n",
      "Managed Spot Training savings: 56.4%\n"
     ]
    }
   ],
   "source": [
    "# This step takes around 35 minutes to train the model with m4.xlarge instance\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job name: deepar-biketrain-with-dynamic-feat-2022-07-04-16-51-30-461\n"
     ]
    }
   ],
   "source": [
    "print ('job name: {0}'.format(job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    }
   ],
   "source": [
    "# Create an endpoint for real-time predictions\n",
    "# SDK 2. parameter name for container: image_uri\n",
    "\n",
    "endpoint_name = sess.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    image_uri=container,\n",
    "    role=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('endpoint name: {0}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next lab, we will use the above endpoint for inference\n",
    "# We will delete the endpoint in the next lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
