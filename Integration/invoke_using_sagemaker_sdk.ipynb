{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke SageMaker Enpoint from outside of AWS environment using SageMaker SDK\n",
    "\n",
    "Model used: XGBoost Bike Rental Prediction Trained in the XGBoost Lectures  \n",
    "  \n",
    "This example uses the IAM user: ml_user_predict. The user was setup in the housekeeping lecture of the course.  \n",
    "\n",
    "Refer to the lecture: Configure IAM Users, Setup Command Line Interface (CLI)\n",
    "\n",
    "Ensure xgboost-biketrain-v1 Endpoint is deployed before running this example  \n",
    "  \n",
    "To create an endpoint using SageMaker Console:  \n",
    "1. Select \"Models\" under \"Inference\" in navigation pane\n",
    "2. Search for model using this prefix: xgboost-biketrain-v1\n",
    "3. Select the latest model and choose create endpoint\n",
    "4. Specify endpoint name as: xgboost-biketrain-v1\n",
    "5. Create a new endpoint configuration\n",
    "6. Create a new endpoint\n",
    "7. After this lab is completed, delete the endpoint to avoid unnecessary charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.95.0.tar.gz (530 kB)\n",
      "     -------------------------------------- 530.1/530.1 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting attrs==20.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.3/49.3 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: boto3<2.0,>=1.20.21 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from sagemaker) (1.24.6)\n",
      "Collecting google-pasta\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from sagemaker) (1.22.4)\n",
      "Collecting protobuf<4.0,>=3.1\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-win_amd64.whl (904 kB)\n",
      "     -------------------------------------- 904.1/904.1 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting protobuf3-to-dict<1.0,>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
      "Collecting importlib-metadata<5.0,>=1.4.0\n",
      "  Using cached importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from sagemaker) (1.4.2)\n",
      "Collecting pathos\n",
      "  Downloading pathos-0.2.9-py3-none-any.whl (76 kB)\n",
      "     ---------------------------------------- 76.9/76.9 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.0.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.6 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from boto3<2.0,>=1.20.21->sagemaker) (1.27.6)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from pandas->sagemaker) (2022.1)\n",
      "Collecting multiprocess>=0.70.13\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.3/132.3 kB 7.6 MB/s eta 0:00:00\n",
      "Collecting ppft>=1.7.6.5\n",
      "  Downloading ppft-1.7.6.5-py2.py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.5/52.5 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting pox>=0.3.1\n",
      "  Downloading pox-0.3.1-py2.py3-none-any.whl (28 kB)\n",
      "Collecting dill>=0.3.5.1\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.8/95.8 kB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\work\\projects\\aws_sagemker\\venv\\lib\\site-packages (from botocore<1.28.0,>=1.27.6->boto3<2.0,>=1.20.21->sagemaker) (1.26.9)\n",
      "Building wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\n",
      "  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.95.0-py2.py3-none-any.whl size=745174 sha256=a99542a12bd0158a5703ac03c3ef5bb29a6e87f326629754f2741b8048cd8830\n",
      "  Stored in directory: c:\\users\\peradu\\appdata\\local\\pip\\cache\\wheels\\d2\\52\\13\\99a22ab07dce1623508743e60c4cbb12ef02afeecfe4053cfd\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4015 sha256=77f8c22b6cdeb3e79730bb23339fe847c2196855442b641648fd7402fc456bdc\n",
      "  Stored in directory: c:\\users\\peradu\\appdata\\local\\pip\\cache\\wheels\\21\\bf\\76\\90dd7b8d0598c7642532062ddff00ecef07df873c36396488c\n",
      "Successfully built sagemaker protobuf3-to-dict\n",
      "Installing collected packages: zipp, smdebug_rulesconfig, protobuf, ppft, pox, google-pasta, dill, attrs, protobuf3-to-dict, multiprocess, importlib-metadata, pathos, sagemaker\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "Successfully installed attrs-20.3.0 dill-0.3.5.1 google-pasta-0.2.0 importlib-metadata-4.11.4 multiprocess-0.70.13 pathos-0.2.9 pox-0.3.1 ppft-1.7.6.5 protobuf-3.20.1 protobuf3-to-dict-0.1.5 sagemaker-2.95.0 smdebug_rulesconfig-1.0.1 zipp-3.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install SageMaker 2.x version.\n",
    "# !pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import math\n",
    "import dateutil\n",
    "import re\n",
    "\n",
    "# SDK 2 serializers and deserializers\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a session with AWS\n",
    "# Specify credentials and region to be used for this session.\n",
    "# We will use a ml_user_predict credentials that has limited privileges\n",
    "boto_session = boto3.Session(profile_name='user_tmd',region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictor and point to an existing endpoint\n",
    "\n",
    "# Get Predictor using SageMaker SDK\n",
    "# Specify Your Endpoint Name\n",
    "endpoint_name = 'xgboost-biketrain-v1'\n",
    "\n",
    "predictor = sagemaker.predictor.Predictor(endpoint_name=endpoint_name,\n",
    "                                                 sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are sending data for inference in CSV format\n",
    "predictor.serializer = CSVSerializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed\n",
    "# Actual=562\n",
    "sample_one = '2012-12-19 17:00:00,4,0,1,1,16.4,20.455,50,26.0027'\n",
    "# Actual=569\n",
    "sample_two = '2012-12-19 18:00:00,4,0,1,1,15.58,19.695,50,23.9994'\n",
    "# Actual=4\n",
    "sample_three = '2012-12-10 01:00:00,4,0,1,2,14.76,18.94,100,0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data Structure: \n",
    "# datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count\n",
    "\n",
    "# Model expects data in this format (it was trained with these features):\n",
    "# season,holiday,workingday,weather,temp,atemp,humidity,windspeed,year,month,day,dayofweek,hour\n",
    "\n",
    "def transform_data(data):\n",
    "    features = data.split(',')\n",
    "    \n",
    "    # Extract year, month, day, dayofweek, hour\n",
    "    dt = dateutil.parser.parse(features[0])\n",
    "\n",
    "    features.append(str(dt.year))\n",
    "    features.append(str(dt.month))\n",
    "    features.append(str(dt.day))\n",
    "    features.append(str(dt.weekday()))\n",
    "    features.append(str(dt.hour))\n",
    "    \n",
    "    # Return the transformed data. skip datetime field\n",
    "    return ','.join(features[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Data:\n",
      " 2012-12-19 17:00:00,4,0,1,1,16.4,20.455,50,26.0027\n",
      "Transformed Data:\n",
      " 4,0,1,1,16.4,20.455,50,26.0027,2012,12,19,2,17\n"
     ]
    }
   ],
   "source": [
    "print('Raw Data:\\n',sample_one)\n",
    "print('Transformed Data:\\n',transform_data(sample_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'573.6282958984375\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's invoke prediction now\n",
    "predictor.predict(transform_data(sample_one))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Count 1.329240521840346e+249\n"
     ]
    }
   ],
   "source": [
    "# Actual Count is 562...but predicted is 6.3.\n",
    "\n",
    "# Model was trained with log1p(count)\n",
    "# So, we need to apply inverse transformation to get the actual count\n",
    "# Predicted Count looks much better now\n",
    "result = predictor.predict(transform_data(sample_one))\n",
    "result = result.decode(\"utf-8\")\n",
    "print ('Predicted Count', math.expm1(float(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to send multiple samples\n",
    "result = predictor.predict([transform_data(sample_one), transform_data(sample_two)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'573.6282958984375\\n547.5216064453125\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Prediction\n",
    "# Transform data and invoke prediction in specified batch sizes\n",
    "def run_predictions(data, batch_size):\n",
    "    predictions = []\n",
    "    \n",
    "    transformed_data = [transform_data(row.strip()) for row in data]\n",
    "    \n",
    "    for i in range(0, len(data), batch_size):\n",
    "        \n",
    "        print(i,i+batch_size)\n",
    "        \n",
    "        result = predictor.predict(transformed_data[i : i + batch_size])\n",
    "        pattern = r'[^0-9.]+'\n",
    "        re.split(pattern, result.decode())\n",
    "#         result = result.decode(\"utf-8\")\n",
    "#         result = result.split(',')\n",
    "        \n",
    "        predictions += [math.expm1(float(r)) for r in result]\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0413759433029089e+23,\n",
       " 7.694785265142018e+23,\n",
       " 1.4093490824269389e+22,\n",
       " 9.496119420602448e+19,\n",
       " 2.830753303274694e+23,\n",
       " 5.184705528587072e+21,\n",
       " 2.091659496012996e+24,\n",
       " 5.184705528587072e+21,\n",
       " 5.685719999335932e+24,\n",
       " 1.0413759433029089e+23,\n",
       " 2.091659496012996e+24,\n",
       " 5.685719999335932e+24,\n",
       " 2.091659496012996e+24,\n",
       " 3.831008000716577e+22,\n",
       " 1.4093490824269389e+22,\n",
       " 7.694785265142018e+23,\n",
       " 1.0413759433029089e+23,\n",
       " 22025.465794806718,\n",
       " 1.0413759433029089e+23,\n",
       " 3.831008000716577e+22,\n",
       " 7.694785265142018e+23,\n",
       " 9.496119420602448e+19,\n",
       " 1.0413759433029089e+23,\n",
       " 5.184705528587072e+21,\n",
       " 1.9073465724950998e+21,\n",
       " 2.830753303274694e+23,\n",
       " 7.016735912097631e+20,\n",
       " 2.830753303274694e+23,\n",
       " 3.831008000716577e+22,\n",
       " 3.831008000716577e+22,\n",
       " 1.0413759433029089e+23,\n",
       " 1.4093490824269389e+22,\n",
       " 1.9073465724950998e+21,\n",
       " 5.184705528587072e+21,\n",
       " 1.0413759433029089e+23,\n",
       " 22025.465794806718,\n",
       " 1.9073465724950998e+21,\n",
       " 7.016735912097631e+20,\n",
       " 9.496119420602448e+19,\n",
       " 3.831008000716577e+22,\n",
       " 5.184705528587072e+21,\n",
       " 1.4093490824269389e+22,\n",
       " 2.091659496012996e+24,\n",
       " 1.9073465724950998e+21,\n",
       " 2.830753303274694e+23,\n",
       " 2.830753303274694e+23,\n",
       " 2.091659496012996e+24,\n",
       " 7.016735912097631e+20,\n",
       " 5.685719999335932e+24,\n",
       " 7.016735912097631e+20,\n",
       " 2.091659496012996e+24,\n",
       " 5.184705528587072e+21,\n",
       " 7.016735912097631e+20,\n",
       " 1.4093490824269389e+22,\n",
       " 22025.465794806718]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_predictions([sample_one,sample_two,sample_three],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a batch prediction on Test.CSV File\n",
    "# Read the file content\n",
    "data = []\n",
    "with open('../data/byke sharing/test.csv','r') as f:\n",
    "    # skip header\n",
    "    f.readline()\n",
    "    # Read remaining lines\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6493"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "500 1000\n",
      "1000 1500\n",
      "1500 2000\n",
      "2000 2500\n",
      "2500 3000\n",
      "3000 3500\n",
      "3500 4000\n",
      "4000 4500\n",
      "4500 5000\n",
      "5000 5500\n",
      "5500 6000\n",
      "6000 6500\n",
      "CPU times: total: 359 ms\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = run_predictions(data,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118771, 6493)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions),len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to delete the endpoint\n",
    "# From SageMaker Console, Select \"Endpoints\" under Inference and Delete the Endpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}